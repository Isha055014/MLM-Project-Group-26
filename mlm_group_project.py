# -*- coding: utf-8 -*-
"""MLM Group PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pds_0SenIvdho-TpWrZI5Feq1CDOLPjC

#PROJECT REPORT

**PROJECT INFORMATION**

Project Title: Unsupervised Learning Model using Python: Clustering

Students Name and Enrollment Numbers: Isha Gupta (055014) | Sneha Gupta (055047)

Group Number: 26


**DESCRIPTION OF DATA**

**Data Source:** https://www.kaggle.com/datasets/chakilamvishwas/imports-exports-15000

**Data Size:**  Approximately 15000 observations and 16 variables

**Data Type:** Cross-Sectional

**Data Dimension:** Number of Variable - 16; Number of Observations - 15,000

**Data Variable Type**:

->**Numeric:**

-> -> **Integer:** Quantity, Customs_Code

-> -> **Decimal:** Value, Weight

-> **Non-Numeric:** Country
Product
Import_Export
Category
Port
Shipping_Method
Supplier
Customer
Payment_Terms
Date
Transaction_ID

**Data Variable Category-I:**

**Index:** Transaction_ID

**Categorical:** Country, Product, Import_Export, Category, Port, Shipping_Method, Supplier, Customer, Payment_Terms

**Non-Categorical:** Quantity, Value, Date, Weight, Customs_Code

**Data Variable Category-II:**

**Input Variables or Features**: All variables for clustering

**Outcome Variable(s) or Feature(s)**: Not applicable (Unsupervised Learning)

**About Dataset:**
The dataset contains several categorical variables that provide descriptive information about the transactions. These include Country, which indicates the nation involved in the trade; Product, specifying the type of goods being imported or exported; and Import_Export, identifying the transaction as either an import or export activity. Other categorical fields such as Category classify the products into broader groups like electronics or clothing, while Port and Shipping_Method describe the logistics of the transaction, including the port of entry/exit and the mode of transport. Additionally, variables like Supplier and Customer highlight the parties involved in the trade, and Payment_Terms outlines the agreed financial conditions. Finally, Date records the transaction's occurrence time, and Transaction_ID uniquely identifies each record in the dataset. These variables will require preprocessing, such as encoding, to be suitable for clustering algorithms.

#### Import Relevant Libraries
"""

import pandas as pd, numpy as np                                                        # For Data Manipulation
import scipy.stats as stats                                                             # Importing the stats module from scipy
from scipy.stats import skew, kurtosis                                                  # For descriptive stats of non-cat variables
from scipy.stats import spearmanr                                                       # For Spearman correlation
from scipy.stats import chi2_contingency                                                # For Chi-sq test
from scipy.stats import t                                                               # For T-test
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder                          # For Encoding Categorical Data [Nominal | Ordinal]
from sklearn.preprocessing import OneHotEncoder                                         # For Creating Dummy Variables of Categorical Data [Nominal]
from sklearn.impute import SimpleImputer, KNNImputer                                    # For Imputation of Missing Data
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler            # For Rescaling Data
from sklearn.model_selection import train_test_split                                    # For Splitting Data into Training & Testing Sets
import matplotlib.pyplot as plt                                                         # For data visualization
from matplotlib import pyplot as pp                                                     # For data visualization
import seaborn as sns                                                                   # For data visualization
from scipy.cluster.hierarchy import dendrogram, linkage                                 # For Hierarchical Clustering
from sklearn.preprocessing import StandardScaler                                        # For Hierarchical Clustering
from sklearn.cluster import KMeans                                                      # For K-Means Clustering
from sklearn.cluster import DBSCAN                                                      # For DBSCAN Clustering
from sklearn.cluster import Birch                                                       # For BIRCH Clustering
import scipy.cluster.hierarchy as sch                                                   # For Hierarchical Clustering
from scipy.cluster.hierarchy import fcluster                                            # For Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering as agclus                           # For Agglomerative & K-Means Clustering
from sklearn.metrics import silhouette_score as sscore, davies_bouldin_score as dbscore # For Clustering Model Evaluation
from sklearn.decomposition import PCA                                                   # For Principal component analysis
from sklearn.model_selection import train_test_split                                    # For data bifurcation
from sklearn.linear_model import LogisticRegression                                     # For Logistic regression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score # For performance metrics
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree                 # For decision tree
from sklearn.ensemble import RandomForestClassifier                                     # For ensembling
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict           # For cross validation

"""#### Upload Live Dataset

We will begin by uploading live data using a Google Drive link. The file ID from the link will be extracted and incorporated into a URL for direct access. The dataset will then be loaded into a DataFrame for further preprocessing and analysis. This approach ensures the data remains up-to-date and accessible without manual intervention.
"""

# File ID from Google Drive link
sg47_file_id = '1nX-OYPadLCWTWIKG0qTP6GVs2C5aZP1r'

# Construct the download URL
sg47_download_url = f'https://drive.google.com/uc?id={sg47_file_id}'

# Load the dataset
sg47_export_import_data = pd.read_csv(sg47_download_url)
sg47_export_import_data

"""#### Creating Sample of 5001 records from Live Dataset"""

sg47_sample_data = sg47_export_import_data.sample(n=5001, random_state=5504714)
sg47_sample_data

# Dimensions of Sample Data
sg47_sample_data.shape

"""#### Data Bifurcation into Categorical and Non-Categorical Variables"""

from tabulate import tabulate

# Data for the table
sg47_table_values = [["Transaction_ID", "Index", ""],
        ["Country", "Categorical", "Nominal"],
        ["Product", "Categorical", "Nominal"],
              ["Import_Export", "Categorical", "Nominal"],
        ["Quantity", "Non-Categorical", ""],
        ["Value", "Non-Categorical", ""],
              ["Date", "Categorical", "Nominal"],
        ["Category", "Categorical", "Nominal"],
        ["Port", "Categorical", "Nominal"],
              ["Customs_Code", "Non-Categorical", ""],
        ["Weight", "Non-Categorical", ""],
        ["Shipping_Method", "Categorical", "Nominal"],
              ["Supplier", "Categorical", "Nominal"],
        ["Customer", "Categorical", "Nominal"],
        ["Invoice_Number", "Non-Categorical", ""],
               ["Payment_Terms", "Categorical", "Nominal"]]

# Column headers
sg47_headers = ["Variable", "Type", "Category"]

# Generate table
sg47_table = tabulate(sg47_table_values, headers=sg47_headers, tablefmt="grid")
print(sg47_table)

# Categorical Data [Nominal | Ordinal]
sg47_cat = sg47_sample_data[['Country', 'Product', 'Import_Export', 'Date', 'Category', 'Port',
                             'Shipping_Method', 'Supplier', 'Customer', 'Payment_Terms']]
sg47_cat

# Non-Categorical Data
sg47_non_cat = sg47_sample_data[['Quantity', 'Value', 'Customs_Code', 'Weight', 'Invoice_Number']]
sg47_non_cat

"""### __Data Preprocessing__
#### Treatment of Missing Data
"""

# Missing Data Information
sg47_sample_data.info()

# Variable-wise Missing Data Information
sg47_variable_missing_data = sg47_sample_data.isna().sum(); sg47_variable_missing_data

# Record-wise Missing Data Information
sg47_record_missing_data = sg47_sample_data.isna().sum(axis=1).sort_values(ascending=False).head(); sg47_record_missing_data

"""##### __Automated Code for Missing Data Handling__"""

# Function to handle missing data dynamically
def handle_missing_data(sg47_sample_data, threshold=0.5, strategy='median'):
    """
    Handles missing data in a DataFrame by either removing columns/rows with >threshold% missing values
    or imputing the missing values using the specified strategy.

    Parameters:
        sg47_sample_data (pd.DataFrame): Input DataFrame.
        threshold (float): Threshold for removing columns/rows (default: 0.5).
        strategy (str): Imputation strategy ('mean', 'median', or 'mode').

    Returns:
        pd.DataFrame: DataFrame after handling missing data.
    """
    # Calculate the percentage of missing values per column
    sg47_missing_percentage = sg47_sample_data.isnull().mean()

    # Step 1: Drop columns with missing data > threshold
    sg47_columns_to_drop = sg47_missing_percentage[sg47_missing_percentage > threshold].index
    sg47_sample_data = sg47_sample_data.drop(columns=sg47_columns_to_drop)
    print(f"Dropped columns with >{threshold * 100}% missing values: {list(sg47_columns_to_drop)}")

    # Step 2: Drop rows with missing data > threshold
    sg47_row_missing_percentage = sg47_sample_data.isnull().mean(axis=1)
    sg47_rows_to_drop = sg47_row_missing_percentage[sg47_row_missing_percentage > threshold].index
    sg47_sample_data = sg47_sample_data.drop(index=sg47_rows_to_drop)
    print(f"Dropped rows with >{threshold * 100}% missing values: {list(sg47_rows_to_drop)}")

    # Step 3: Impute remaining missing values based on the strategy
    for col in sg47_sample_data.columns:
        if sg47_sample_data[col].isnull().sum() > 0:
            if strategy == 'mean' and pd.api.types.is_numeric_dtype(sg47_sample_data[col]):
                sg47_sample_data[col] = sg47_sample_data[col].fillna(sg47_sample_data[col].mean())
            elif strategy == 'median' and pd.api.types.is_numeric_dtype(sg47_sample_data[col]):
                sg47_sample_data[col] = sg47_sample_data[col].fillna(sg47_sample_data[col].median())
            elif strategy == 'mode':
                sg47_sample_data[col] = sg47_sample_data[col].fillna(sg47_sample_data[col].mode()[0])  # Mode always returns the first mode value
            print(f"Imputed missing values in column '{col}' using '{strategy}'")

    return sg47_sample_data

"""#### Numerical Encoding of Non-Numeric Categorical Data"""

# Using Scikit Learn : Ordinal Encoder (Superior)
sg47_oe = OrdinalEncoder()

# Apply OrdinalEncoder on each column
sg47_oe_fit1 = sg47_oe.fit_transform(sg47_cat[['Import_Export']])
sg47_oe_fit2 = sg47_oe.fit_transform(sg47_cat[['Category']])
sg47_oe_fit3 = sg47_oe.fit_transform(sg47_cat[['Shipping_Method']])
sg47_oe_fit4 = sg47_oe.fit_transform(sg47_cat[['Payment_Terms']])

# Create DataFrames for encoded columns
sg47_encoded1 = pd.DataFrame(sg47_oe_fit1, columns=['Import_Export_code'])
sg47_encoded2 = pd.DataFrame(sg47_oe_fit2, columns=['Category_code'])
sg47_encoded3 = pd.DataFrame(sg47_oe_fit3, columns=['Shipping_Method_code'])
sg47_encoded4 = pd.DataFrame(sg47_oe_fit4, columns=['Payment_Terms_code'])

# Reset the index of each DataFrame to ensure alignment
sg47_encoded1.reset_index(drop=True, inplace=True)
sg47_encoded2.reset_index(drop=True, inplace=True)
sg47_encoded3.reset_index(drop=True, inplace=True)
sg47_encoded4.reset_index(drop=True, inplace=True)

# Concatenate the new encoded columns
sg47_encoded_codes = pd.concat([
    sg47_encoded1,
    sg47_encoded2,
    sg47_encoded3,
    sg47_encoded4], axis=1)

# Join the sample with the encoded columns
sg47_cat_codes = sg47_sample_data.reset_index(drop=True).join(sg47_encoded_codes)
sg47_cat_codes

"""##### __Encoded Codes:__
**Import_Export** : Export = 0, Import = 1

**Category** : Clothing = 0, Electronics = 1, Furniture = 2, Machine = 3, Toys = 4

**Shipping_Method** : Air = 0, Land = 1, Sea =2

**Payment_Terms** : Cash on Delivery = 0, Net 30 = 1, Net 60 = 2, Prepaid = 3
"""

# Summary of all encoded codes
sg47_encoded_codes

"""#### Re-Scaling & Transformation of Numeric Data (Treatment of Data having Outliers)

#### _Standardization_
"""

# Initialize the StandardScaler
sg47_ss = StandardScaler()

# Fit and transform the selected columns
sg47_ss_fit = sg47_ss.fit_transform(sg47_non_cat[['Quantity', 'Value', 'Weight']])

# Create a DataFrame with standardized columns
sg47_noncat_std = pd.DataFrame(sg47_ss_fit, columns=['Quantity_std', 'Value_std', 'Weight_std'], index=sg47_non_cat.index)

# Concatenate the standardized columns back into the original non-categorical dataset
sg47_noncat_std_combined = pd.concat([sg47_non_cat, sg47_noncat_std], axis=1)

# Display the updated DataFrame
sg47_noncat_std_combined

"""#### _Normalization (Min-Max Scaling)_"""

# Initialize the MinMaxScaler
sg47_mms = MinMaxScaler()

# Fit and transform the selected columns
sg47_mms_fit = sg47_mms.fit_transform(sg47_non_cat[['Quantity', 'Value', 'Weight']])

# Create a DataFrame with MinMax normalized columns
sg47_noncat_minmax_norm = pd.DataFrame(sg47_mms_fit, columns=['Quantity_mmnorm', 'Value_mmnorm', 'Weight_mmnorm'], index=sg47_non_cat.index)

# Concatenate the normalized columns back into the original non-categorical dataset
sg47_noncat_mmn_combined = pd.concat([sg47_non_cat, sg47_noncat_minmax_norm], axis=1)

# Display the updated DataFrame
sg47_noncat_mmn_combined

"""#### _Log Transformation_"""

# Create a copy of the non-categorical DataFrame
sg47_lt = sg47_non_cat.copy()

# Apply log transformation to the selected columns
sg47_lt[['Quantity_lt', 'Value_lt', 'Weight_lt']] = sg47_lt[['Quantity', 'Value', 'Weight']].apply(np.log)

# Display the DataFrame with log-transformed columns
sg47_lt

"""##### Final Preprocessed Subset"""

# Reset indices or align indices before joining
sg47_encoded_codes.reset_index(drop=True, inplace=True)
sg47_noncat_minmax_norm.reset_index(drop=True, inplace=True)

# Join the DataFrames
sg47_ppd = sg47_encoded_codes.join(sg47_noncat_minmax_norm)
sg47_ppd

"""### __Descriptive Statistics__

"""

sg47_sample_data.describe()                          # No Meaningful Information regarding Categorical Variables

"""#### _Categorical Variable [Nominal | Ordinal]_
##### Count Statistics | Frequency | Proportion
"""

# 'Country'
sg47_country_stats = pd.concat([sg47_sample_data['Country'].value_counts(), sg47_sample_data['Country'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_country_stats

# 'Product'
sg47_product_stats = pd.concat([sg47_sample_data['Product'].value_counts(), sg47_sample_data['Product'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_product_stats

# 'Import_Export'
sg47_imp_exp_stats = pd.concat([sg47_sample_data['Import_Export'].value_counts(), sg47_sample_data['Import_Export'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_imp_exp_stats

# 'Date'
sg47_date_stats = pd.concat([sg47_sample_data['Date'].value_counts(), sg47_sample_data['Date'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_date_stats

# 'Category'
sg47_category_stats = pd.concat([sg47_sample_data['Category'].value_counts(), sg47_sample_data['Category'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_category_stats

# 'Port'
sg47_port_stats = pd.concat([sg47_sample_data['Port'].value_counts(), sg47_sample_data['Port'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_port_stats

# 'Shipping_Method'
sg47_shipping_stats = pd.concat([sg47_sample_data['Shipping_Method'].value_counts(), sg47_sample_data['Shipping_Method'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_shipping_stats

# 'Supplier'
sg47_supplier_stats = pd.concat([sg47_sample_data['Supplier'].value_counts(), sg47_sample_data['Supplier'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_supplier_stats

# 'Customer'
sg47_customer_stats = pd.concat([sg47_sample_data['Customer'].value_counts(), sg47_sample_data['Customer'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_customer_stats

# 'Payment_Terms'
sg47_pay_stats = pd.concat([sg47_sample_data['Payment_Terms'].value_counts(), sg47_sample_data['Payment_Terms'].value_counts(normalize=True).mul(100).round(2)],
                               axis=1, keys=('Count','Percentage')).reset_index()
sg47_pay_stats

"""##### Minimum | Maximum | Mode | Rank"""

# 'Country'
sg47_country_min = sg47_sample_data['Country'].min()
sg47_country_max = sg47_sample_data['Country'].max()
sg47_country_mode = sg47_sample_data['Country'].mode()[0]
sg47_country_rank = sg47_sample_data['Country'].value_counts().rank(method='min', ascending=False)

print('Minimum Country:',sg47_country_min)
print('Maximum Country:',sg47_country_max)
print('Mode Country:',sg47_country_mode)
print('Rank:',sg47_country_rank)

# 'Product'
sg47_product_min = sg47_sample_data['Product'].min()
sg47_product_max = sg47_sample_data['Product'].max()
sg47_product_mode = sg47_sample_data['Product'].mode()[0]
sg47_product_rank = sg47_sample_data['Product'].value_counts().rank(method='min', ascending=False)

print('Minimum Product:',sg47_product_min)
print('Maximum Product:',sg47_product_max)
print('Mode Product:',sg47_product_mode)
print('Rank:',sg47_product_rank)

# 'Import_Export'
sg47_imp_exp_min = sg47_sample_data['Import_Export'].min()
sg47_imp_exp_max = sg47_sample_data['Import_Export'].max()
sg47_imp_exp_mode = sg47_sample_data['Import_Export'].mode()[0]
sg47_imp_exp_rank = sg47_sample_data['Import_Export'].value_counts().rank(method='min', ascending=False)

print('Minimum Import_Export:',sg47_imp_exp_min)
print('Maximum Import_Export:',sg47_imp_exp_max)
print('Mode Import_Export:',sg47_imp_exp_mode)
print('Rank:',sg47_imp_exp_rank)

# 'Date'
sg47_date_min = sg47_sample_data['Date'].min()
sg47_date_max = sg47_sample_data['Date'].max()
sg47_date_mode = sg47_sample_data['Date'].mode()[0]
sg47_date_rank = sg47_sample_data['Date'].value_counts().rank(method='min', ascending=False)

print('Minimum Date:',sg47_date_min)
print('Maximum Date:',sg47_date_max)
print('Mode Date:',sg47_date_mode)
print('Rank:',sg47_date_rank)

# 'Category'
sg47_category_min = sg47_sample_data['Category'].min()
sg47_category_max = sg47_sample_data['Category'].max()
sg47_category_mode = sg47_sample_data['Category'].mode()[0]
sg47_category_rank = sg47_sample_data['Category'].value_counts().rank(method='min', ascending=False)

print('Minimum Category:',sg47_category_min)
print('Maximum Category:',sg47_category_max)
print('Mode Category:',sg47_category_mode)
print('Rank:',sg47_category_rank)

# 'Port'
sg47_port_min = sg47_sample_data['Port'].min()
sg47_port_max = sg47_sample_data['Port'].max()
sg47_port_mode = sg47_sample_data['Port'].mode()[0]
sg47_port_rank = sg47_sample_data['Port'].value_counts().rank(method='min', ascending=False)

print('Minimum Port:',sg47_port_min)
print('Maximum Port:',sg47_port_max)
print('Mode Port:',sg47_port_mode)
print('Rank:',sg47_port_rank)

# 'Shipping_Method'
sg47_shipping_min = sg47_sample_data['Shipping_Method'].min()
sg47_shipping_max = sg47_sample_data['Shipping_Method'].max()
sg47_shipping_mode = sg47_sample_data['Shipping_Method'].mode()[0]
sg47_shipping_rank = sg47_sample_data['Shipping_Method'].value_counts().rank(method='min', ascending=False)

print('Minimum Shipping_Method:',sg47_shipping_min)
print('Maximum Shipping_Method:',sg47_shipping_max)
print('Mode Shipping_Method:',sg47_shipping_mode)
print('Rank:',sg47_shipping_rank)

# 'Supplier'
sg47_supplier_min = sg47_sample_data['Supplier'].min()
sg47_supplier_max = sg47_sample_data['Supplier'].max()
sg47_supplier_mode = sg47_sample_data['Supplier'].mode()[0]
sg47_supplier_rank = sg47_sample_data['Supplier'].value_counts().rank(method='min', ascending=False)

print('Minimum Supplier:',sg47_supplier_min)
print('Maximum Supplier:',sg47_supplier_max)
print('Mode Supplier:',sg47_supplier_mode)
print('Rank:',sg47_supplier_rank)

# 'Customer'
sg47_customer_min = sg47_sample_data['Customer'].min()
sg47_customer_max = sg47_sample_data['Customer'].max()
sg47_customer_mode = sg47_sample_data['Customer'].mode()[0]
sg47_customer_rank = sg47_sample_data['Customer'].value_counts().rank(method='min', ascending=False)

print('Minimum Customer:',sg47_customer_min)
print('Maximum Customer:',sg47_customer_max)
print('Mode Customer:',sg47_customer_mode)
print('Rank:',sg47_customer_rank)

# 'Payment_Terms'
sg47_pay_min = sg47_sample_data['Payment_Terms'].min()
sg47_pay_max = sg47_sample_data['Payment_Terms'].max()
sg47_pay_mode = sg47_sample_data['Payment_Terms'].mode()[0]
sg47_pay_rank = sg47_sample_data['Payment_Terms'].value_counts().rank(method='min', ascending=False)

print('Minimum Payment_Terms:',sg47_pay_min)
print('Maximum Payment_Terms:',sg47_pay_max)
print('Mode Payment_Terms:',sg47_pay_mode)
print('Rank:',sg47_pay_rank)

"""##### Correlation (Spearman | Kendall)"""

# Spearman Correlation between Shipping_Method and Payment_Terms
sg47_spearman_corr1, sg47_p_value1 = stats.spearmanr(sg47_encoded_codes.iloc[:, 2], sg47_encoded_codes.iloc[:, 3])
print(f"Spearman Correlation between Shipping_Method and Payment_Terms: {sg47_spearman_corr1}, P-value: {sg47_p_value1}")

# Spearman Correlation between Import_Export and Shipping_Method
sg47_spearman_corr2, sg47_p_value2 = stats.spearmanr(sg47_encoded_codes.iloc[:, 0], sg47_encoded_codes.iloc[:, 2])
print(f"Spearman Correlation between Import_Export and Shipping_Method: {sg47_spearman_corr2}, P-value: {sg47_p_value2}")

# Kendall correlation between Import_Export and Payment_Terms
sg47_kendall_corr, sg47_p_value_kendall = stats.kendalltau(sg47_encoded_codes.iloc[:, 0], sg47_encoded_codes.iloc[:, 3])
print(f"Kendall Correlation between Import_Export and Payment_Terms: {sg47_kendall_corr}, P-value: {sg47_p_value_kendall}")

"""#### _Non-Categorical Variable_

##### Measures of Central Tendency {Minimum | Maximum | Mean | Median | Mode | Percentile}

##### Measures of Dispersion {Range | Standard Deviation | Skewness | Kurtosis}

##### Composite Measures {Coefficient of Variation | Confidence Interval}
"""

sg47_noncat_descriptives = sg47_non_cat.describe()
sg47_noncat_descriptives

# Calculate Mode for each column
sg47_mode = sg47_non_cat.mode().iloc[0]

# Calculate Percentiles (25th, 50th, 75th)
sg47_percentiles = sg47_non_cat.quantile([0.25, 0.5, 0.75])

# Calculate Range (Max - Min) for each column
sg47_range = sg47_non_cat.max() - sg47_non_cat.min()

# Calculate Skewness for each column
sg47_skewness = sg47_non_cat.apply(skew)

# Calculate Kurtosis for each column
sg47_kurtosis = sg47_non_cat.apply(kurtosis)

# Calculate Coefficient of Variation (CV) for each column
sg47_cv = sg47_non_cat.std() / sg47_non_cat.mean()

# Calculate Confidence Interval (CI) for each column
sg47_n = 5001                 # Sample size
sg47_ci_lower = sg47_non_cat.mean() - 1.96 * (sg47_non_cat.std() / np.sqrt(sg47_n))
sg47_ci_upper = sg47_non_cat.mean() + 1.96 * (sg47_non_cat.std() / np.sqrt(sg47_n))

# Combine all of the above into a new DataFrame for easy addition to existing descriptives
sg47_summary_stats = pd.DataFrame({
    'Mode': sg47_mode,
    '25th Percentile': sg47_percentiles.loc[0.25],
    '50th Percentile': sg47_percentiles.loc[0.5],
    '75th Percentile': sg47_percentiles.loc[0.75],
    'Range': sg47_range,
    'Skewness': sg47_skewness,
    'Kurtosis': sg47_kurtosis,
    'Coefficient of Variation': sg47_cv,
    'CI Lower Bound': sg47_ci_lower,
    'CI Upper Bound': sg47_ci_upper
})

# Transpose the summary stats to add them as rows
sg47_summary_stats_transposed = sg47_summary_stats.T

# Concatenate the existing descriptive statistics with the new rows
sg47_descriptives_final = pd.concat([sg47_noncat_descriptives, sg47_summary_stats_transposed])
sg47_descriptives_final

"""##### Measures of Dispersion {Correlation (Pearson | Spearman)}"""

# Pearson Correlation
sg47_pearson_corr = sg47_non_cat.corr(method='pearson')
print("Pearson Correlation:")
sg47_pearson_corr

# Spearman Correlation
sg47_spearman_corr, _ = spearmanr(sg47_non_cat)
print("Spearman Correlation:")
print(sg47_spearman_corr)

"""### __Data Visualization__
#### _Basic Plots - Bar | Pie | Scatter | Line_

"""

# Bar chart
sns.countplot(x='Import_Export', data=sg47_sample_data)
plt.title('Number of Imports/Exports')
plt.xticks(rotation=45)
plt.show()

# Pie chart
sg47_shipping_method_counts = sg47_sample_data['Shipping_Method'].value_counts()
plt.pie(sg47_shipping_method_counts, labels=sg47_shipping_method_counts.index, autopct='%1.1f%%')
plt.title('Distribution of Shipping Methods')
plt.show()

# Pie chart
sg47_payment_terms_counts = sg47_sample_data['Payment_Terms'].value_counts()
plt.pie(sg47_payment_terms_counts, labels=sg47_payment_terms_counts.index, autopct='%1.1f%%')
plt.title('Distribution of Payment Terms')
plt.show()

# Scatter plot
sns.scatterplot(x='Shipping_Method_code', y='Import_Export_code', data=sg47_ppd, hue='Quantity_mmnorm', palette='coolwarm', style='Payment_Terms_code', markers=["o", "s", "d", "v"])
plt.title('Shipping Method vs Import Export (By Quantity)', fontsize=14)
plt.xlabel('Shipping Method', fontsize=12)
plt.ylabel('Import Export', fontsize=12)
plt.legend(title='Scale:', loc='upper right')
plt.show()

# Line Plot
sg47_category_line = sg47_ppd.groupby('Category_code')[['Quantity_mmnorm', 'Value_mmnorm']].sum()

# Creating the line plot
plt.figure(figsize=(10,6))
sns.lineplot(data=sg47_category_line, x=sg47_category_line.index, y='Quantity_mmnorm', label='Quantity', marker='o')
sns.lineplot(data=sg47_category_line, x=sg47_category_line.index, y='Value_mmnorm', label='Value', marker='o')

# Adding labels and title
plt.title('Total Quantity and Value by Category', fontsize=14)
plt.xlabel('Category', fontsize=12)
plt.ylabel('Total', fontsize=12)
plt.legend(title='Variables', loc='upper left')
plt.grid(True)
plt.show()

"""#### _Advance Plots -  Box-Whisker | Pair | Heat_

"""

# Box plot
pp.boxplot([sg47_ppd.Category_code], meanline=True, showmeans=True, labels=['Category'], vert=True)
pp.title('Category Box Plot')
pp.show()

# Pair Plot
sg47_numerical_columns = ['Category_code', 'Shipping_Method_code', 'Payment_Terms_code']

# Creating pairplot
sns.pairplot(sg47_encoded_codes[sg47_numerical_columns], kind='scatter', diag_kind='kde', plot_kws={'alpha':0.6})

# Show the plot
plt.suptitle('Pairplot of Encoded Variables', fontsize=16, y=1.02)
plt.show()

# Correlation heatmap for numerical variables
correlation_matrix = sg47_sample_data[['Quantity', 'Value', 'Weight']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

"""### __Inferential Statistics__

#### _Categorical Variable [Nominal | Ordinal]_
##### Test of Homogeneity {Chi-sq}
"""

# Contingency table
# Comparing 'Import_Export' and 'Shipping_Method'
sg47_contingency_table = pd.crosstab(sg47_sample_data['Import_Export'], sg47_sample_data['Shipping_Method'])

# Perform the Chi-Square test
sg47_chi2_stat, sg47_p_value, sg47_dof, sg47_expected = chi2_contingency(sg47_contingency_table)

# Print the results
print("Chi-Square Test of Homogeneity")
print(f"Chi-Square Statistic: {sg47_chi2_stat}")
print(f"P-Value: {sg47_p_value}")
print(f"Degrees of Freedom: {sg47_dof}")
print("\nExpected Frequencies:")
print(sg47_expected)

# Interpret the results
sg47_alpha = 0.05                 # Significance level
if sg47_p_value < sg47_alpha:
    print("\nResult: There is a significant difference in the distribution of 'Shipping_Method' across 'Import_Export'.")
else:
    print("\nResult: There is no significant difference in the distribution of 'Shipping_Method' across 'Import_Export'.")

"""#### _Non-Categorical Variable_

##### Test of Normality {Shapiro-Wilk | Kolmogorov-Smirnov | Anderson-Darling | Jarque-Bera}
"""

# Select the numerical variables from your dataset
sg47_numerical_data = sg47_sample_data[['Quantity', 'Value', 'Weight']]

# Define a function to perform all normality tests
def normality_tests(data):
    results = []

    for col in data.columns:
        # Shapiro-Wilk Test
        sg47_shapiro_stat, sg47_shapiro_p = stats.shapiro(data[col])

        # Kolmogorov-Smirnov Test
        sg47_ks_stat, sg47_ks_p = stats.kstest(data[col], 'norm', args=(data[col].mean(), data[col].std()))

        # Anderson-Darling Test
        sg47_ad_result = stats.anderson(data[col], dist='norm')

        # Jarque-Bera Test
        sg47_jb_stat, sg47_jb_p = stats.jarque_bera(data[col])

        # Store results
        results.append({
            'Variable': col,
            'Shapiro-Wilk (Statistic)': sg47_shapiro_stat,
            'Shapiro-Wilk (P-Value)': sg47_shapiro_p,
            'Kolmogorov-Smirnov (Statistic)': sg47_ks_stat,
            'Kolmogorov-Smirnov (P-Value)': sg47_ks_p,
            'Anderson-Darling (Statistic)': sg47_ad_result.statistic,
            'Anderson-Darling (Critical-Values)': sg47_ad_result.critical_values,
            'Anderson-Darling (Significance Levels)': sg47_ad_result.significance_level,
            'Jarque-Bera (Statistic)': sg47_jb_stat,
            'Jarque-Bera (P-Value)': sg47_jb_p
        })

    return pd.DataFrame(results)

# Perform normality tests
sg47_normality_results = normality_tests(sg47_numerical_data)

# Display the results
print(sg47_normality_results)

"""##### Test of Correlation {t}"""

# Select numerical variables for the test
sg47_numerical_data = sg47_sample_data[['Quantity', 'Value', 'Weight']]
# Define a function to calculate the t-test for correlation
def correlation_t_test(data):
    results = []

    # Loop through pairs of numerical variables
    for i, col1 in enumerate(data.columns):
        for j, col2 in enumerate(data.columns):
            if i < j:        # Avoid duplicate pairs
                # Pearson Correlation Coefficient
                sg47_r = data[col1].corr(data[col2])

                # Degrees of Freedom
                sg47_n = len(data[col1])
                sg47_df = sg47_n - 2

                # Calculate t-statistic
                sg47_t_stat = sg47_r * np.sqrt(sg47_df / (1 - sg47_r**2))

                # Calculate p-value
                sg47_p_value = 2 * (1 - t.cdf(abs(sg47_t_stat), sg47_df))

                # Store results
                results.append({
                    'Variable 1': col1,
                    'Variable 2': col2,
                    'Correlation Coefficient (r)': sg47_r,
                    't-Statistic': sg47_t_stat,
                    'p-Value': sg47_p_value})

    return pd.DataFrame(results)

# Perform the t-test for correlation
sg47_correlation_results = correlation_t_test(sg47_numerical_data)

# Display the results
print(sg47_correlation_results)

"""### __Machine Learning Models__

#### _Dimensity Reduction: Records_
##### Clustering {Hierarchical (H) | K-Means (KM) | (Others: DBSCAN | BIRCH)}

##### **Hierarchical Clustering**
Objective: Identify a hierarchy of non-categorical clusters to understand the nested relationships among data points.
"""

sg47_cluster_data = sg47_sample_data[['Quantity', 'Value', 'Weight']]

# Standardize the data
sg47_scaler = StandardScaler()
sg47_cluster_scaled = sg47_scaler.fit_transform(sg47_cluster_data)

# Perform hierarchical clustering
sg47_linked = linkage(sg47_cluster_scaled, method='ward')
sg47_linked

# Number of clusters to extract
sg47_n_clusters = 3

# Generate cluster labels based on the linkage matrix and desired number of clusters
sg47_cluster_labels = fcluster(sg47_linked, t=sg47_n_clusters, criterion='maxclust')

# Create a DataFrame with the cluster labels
sg47_hierarchical_clusters = pd.DataFrame({
    'Record_ID': sg47_sample_data.index,  # Replace with a relevant identifier
    'Cluster': sg47_cluster_labels})

sg47_hierarchical_clusters

# Plot dendrogram
plt.figure(figsize=(10, 7))
dendrogram(sg47_linked, orientation='top', distance_sort='descending', show_leaf_counts=False)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Euclidean Distance")
plt.show()

"""##### **Silhouette Coefficient | Davies-Bouldin Index**"""

# Silhouette Score of Hierarchical cluster
sg47_sscore = sscore(sg47_encoded_codes, sg47_cluster_labels); sg47_sscore

# Davies-Bouldin Score of Hierarchical cluster
sg47_dbscore = dbscore(sg47_encoded_codes, sg47_cluster_labels); sg47_dbscore

"""##### **K-Means Clustering**
Objective: Group data into 5 clusters based on proximity in feature space.
"""

# Number of clusters
sg47_k = 5

# Initialize and fit the KMeans model
sg47_kmeans = KMeans(n_clusters=sg47_k, random_state=5504714)
sg47_kmeans_labels = sg47_kmeans.fit_predict(sg47_cluster_scaled)

# Add cluster labels to the original dataset
sg47_sample_data['Cluster'] = sg47_kmeans_labels

# Display the dataset with cluster labels
print(sg47_sample_data.head())

# Plot K-Means Clusters
sns.scatterplot(x=sg47_sample_data['Value'], y=sg47_sample_data['Weight'], hue=sg47_sample_data['Cluster'], palette='viridis')
plt.title("K-Means Clustering")
plt.xlabel("Value")
plt.ylabel("Weight")
plt.legend(title="Cluster")
plt.show()

"""#### **DBSCAN Clustering** (Density-Based Spatial Clustering)
Objective: Identify clusters of varying density and detect outliers.
"""

# Perform DBSCAN clustering
sg47_dbscan = DBSCAN(eps=0.5, min_samples=5)
sg47_dbscan_labels = sg47_dbscan.fit_predict(sg47_cluster_scaled)

# Add DBSCAN labels to the dataset
sg47_sample_data['DBSCAN_Cluster'] = sg47_dbscan_labels

# Display the dataset with DBSCAN cluster labels
print(sg47_sample_data.head())

# Plot DBSCAN Clusters
sns.scatterplot(x=sg47_sample_data['Quantity'], y=sg47_sample_data['Value'], hue=sg47_sample_data['DBSCAN_Cluster'], palette='tab10')
plt.title("DBSCAN Clustering")
plt.xlabel("Quantity")
plt.ylabel("Value")
plt.legend(title="Cluster")
plt.show()

"""##### **BIRCH Clustering**
Objective: Efficiently cluster large datasets.
"""

# Perform BIRCH clustering
sg47_birch = Birch(n_clusters=5)
sg47_birch_labels = sg47_birch.fit_predict(sg47_cluster_scaled)

# Add BIRCH labels to the dataset
sg47_sample_data['BIRCH_Cluster'] = sg47_birch_labels

# Plot BIRCH Clusters
sns.scatterplot(x=sg47_sample_data['Quantity'], y=sg47_sample_data['Weight'], hue=sg47_sample_data['BIRCH_Cluster'], palette='coolwarm')
plt.title("BIRCH Clustering")
plt.xlabel("Quantity")
plt.ylabel("Weight")
plt.legend(title="Cluster")
plt.show()

"""#### _Dimensity Reduction: Variables_
##### Principal Component Analysis
"""

# Select numeric columns for PCA
sg47_numeric_data_pca = sg47_sample_data.select_dtypes(include=[np.number])

# Standardize the data
sg47_scaler = StandardScaler()
sg47_scaled_data = sg47_scaler.fit_transform(sg47_numeric_data_pca)

# Perform PCA
sg47_pca = PCA()
sg47_pca_data = sg47_pca.fit_transform(sg47_scaled_data)

# Explained variance ratio
sg47_explained_variance_ratio = sg47_pca.explained_variance_ratio_
sg47_cumulative_variance = np.cumsum(sg47_explained_variance_ratio)

# Plot explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(sg47_cumulative_variance) + 1), sg47_cumulative_variance, marker='o', linestyle='--')
plt.title('Cumulative Explained Variance')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid()
plt.show()

# Analyze principal components
sg47_pca_df = pd.DataFrame(sg47_pca_data, columns=[f'PC{i+1}' for i in range(sg47_pca_data.shape[1])])
print(sg47_pca_df.head())

"""#### _Supervised Machine Learning_
#### Classification & Regression

##### **Regression {Logistic (LR)}** (Classification Report | Confusion Matrix)
"""

sg47_y = sg47_encoded_codes['Import_Export_code']
# Split the dataset into training and testing sets
sg47_encoded_codes_train, sg47_encoded_codes_test, sg47_y_train, sg47_y_test = train_test_split(sg47_encoded_codes, sg47_y, test_size=0.2, random_state=5504714)

# Train Logistic Regression
sg47_model = LogisticRegression()
sg47_model.fit(sg47_encoded_codes_train, sg47_y_train)

# Make predictions
sg47_y_pred = sg47_model.predict(sg47_encoded_codes_test)

# Evaluate the model
print("Accuracy:", accuracy_score(sg47_y_test, sg47_y_pred))
print("\nClassification Report:\n", classification_report(sg47_y_test, sg47_y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(sg47_y_test, sg47_y_pred))

"""##### **Decision Trees** (Classification Report | Confusion Matrix)"""

sg47_z = sg47_encoded_codes['Category_code']
# Split the dataset into training and testing sets
sg47_encoded_codes_train, sg47_encoded_codes_test, sg47_z_train, sg47_z_test = train_test_split(sg47_encoded_codes, sg47_z, test_size=0.2, random_state=5504714)

# Train a Decision Tree Classifier
sg47_clf = DecisionTreeClassifier(random_state=5504714)
sg47_clf.fit(sg47_encoded_codes_train, sg47_z_train)

# Make predictions
sg47_z_pred = sg47_clf.predict(sg47_encoded_codes_test)

# Evaluate the model
print("Accuracy:", accuracy_score(sg47_z_test, sg47_z_pred))
print("\nClassification Report:\n", classification_report(sg47_z_test, sg47_z_pred))
print("\nConfusion Matrix:\n", confusion_matrix(sg47_z_test, sg47_z_pred))

# Visualize the Decision Tree
plt.figure(figsize=(10, 5))

# Ensure feature names are valid column names from the training data
plot_tree(
    sg47_clf,
    feature_names=list(sg47_encoded_codes.columns),  # Convert columns to a list
    class_names=[str(cls) for cls in sg47_clf.classes_],  # Ensure classes are strings
    filled=True)

plt.title("Decision Tree Visualization")
plt.show()

"""##### Ensembles (Bagging | Boosting): **Random Forest**"""

sg47_a = sg47_ppd
sg47_b = sg47_ppd['Category_code']
# Split the dataset into training and testing sets
sg47_a_train, sg47_a_test, sg47_b_train, sg47_b_test = train_test_split(sg47_a, sg47_b, test_size=0.25, random_state=5504714)

# Train a Random Forest Classifier
sg47_rf_clf = RandomForestClassifier(n_estimators=100, random_state=5504714)
sg47_rf_clf.fit(sg47_a_train, sg47_b_train)

# Make predictions
sg47_b_pred = sg47_rf_clf.predict(sg47_a_test)
sg47_b_pred_proba = sg47_rf_clf.predict_proba(sg47_a_test)[:, 1]              # For AUC-ROC if needed

# Evaluate the model
print("Accuracy:", accuracy_score(sg47_b_test, sg47_b_pred))
print("\nClassification Report:\n", classification_report(sg47_b_test, sg47_b_pred))
print("\nConfusion Matrix:\n", confusion_matrix(sg47_b_test, sg47_b_pred))

# Plot Feature Importances
feature_importances = pd.DataFrame({
    'Feature': sg47_a.columns,
    'Importance': sg47_rf_clf.feature_importances_}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances')
plt.show()

"""Cross-Validation (K-Fold)"""

# Perform K-Fold Cross-Validation
sg47_k = 5                                  # Number of folds
sg47_kf = KFold(n_splits=sg47_k, shuffle=True, random_state=5504714)

# Collect metrics
sg47_fold_accuracies = []
sg47_b_pred_all = []

for train_index, test_index in sg47_kf.split(sg47_a):
    # Split the data
    sg47_a_train, sg47_a_test = sg47_a.iloc[train_index], sg47_a.iloc[test_index]
    sg47_b_train, sg47_b_test = sg47_b.iloc[train_index], sg47_b.iloc[test_index]

    # Train the model
    sg47_rf_clf.fit(sg47_a_train, sg47_b_train)

    # Make predictions
    sg47_b_pred = sg47_rf_clf.predict(sg47_a_test)
    sg47_b_pred_all.extend(sg47_b_pred)

    # Evaluate accuracy for the current fold
    sg47_accuracy = accuracy_score(sg47_b_test, sg47_b_pred)
    sg47_fold_accuracies.append(sg47_accuracy)

# Average accuracy across all folds
sg47_average_accuracy = np.mean(sg47_fold_accuracies)
print(f"K-Fold Cross-Validation Results ({sg47_k} Folds):")
print(f"Fold Accuracies: {sg47_fold_accuracies}")
print(f"Average Accuracy: {sg47_average_accuracy:.4f}")

"""-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"""